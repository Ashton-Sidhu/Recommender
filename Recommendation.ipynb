{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Recommender for MovieLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting MovieLens data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download the movielens 100k dataset from this link: [ml-100k.zip](http://files.grouplens.org/datasets/movielens/ml-100k.zip)\n",
    "\n",
    "* Upload ml-100k.zip using \"My Data\" to /resources/data \n",
    "\n",
    "* Extract using the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /resources/data/ml-100k.zip\r\n",
      "replace /resources/data/ml-100k/allbut.pl? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!unzip /resources/data/ml-100k.zip -d /resources/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from heapq import nlargest\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import os.path\n",
    "import scipy\n",
    "import scipy.stats as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define constant for movie lends 100K directory\n",
    "MOVIELENS_DIR = \"/resources/data/ml-100k/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we inspect the directory content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allbut.pl  u1.base  u2.test  u4.base  u5.test  ub.base\tu.genre  u.occupation\r\n",
      "mku.sh\t   u1.test  u3.base  u4.test  ua.base  ub.test\tu.info\t u.user\r\n",
      "README\t   u2.base  u3.test  u5.base  ua.test  u.data\tu.item\r\n"
     ]
    }
   ],
   "source": [
    "!ls $MOVIELENS_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the full MovieLens 100K dataset to find the number of users and items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  itemID  rating  timestamp\n",
       "0     196     242       3  881250949\n",
       "1     186     302       3  891717742\n",
       "2      22     377       1  878887116\n",
       "3     244      51       2  880606923\n",
       "4     166     346       1  886397596"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields = ['userID', 'itemID', 'rating', 'timestamp']\n",
    "ratingDF = pd.read_csv(os.path.join(MOVIELENS_DIR, 'u.data'), sep='\\t', names=fields)\n",
    "\n",
    "ratingDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 943\n",
      "Number of items: 1682\n"
     ]
    }
   ],
   "source": [
    "numUsers = len(ratingDF.userID.unique())\n",
    "numItems = len(ratingDF.itemID.unique())\n",
    "\n",
    "print(\"Number of users:\", numUsers)\n",
    "print(\"Number of items:\", numItems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieID</th>\n",
       "      <th>movieTitle</th>\n",
       "      <th>releaseDate</th>\n",
       "      <th>videoReleaseDate</th>\n",
       "      <th>IMDbURL</th>\n",
       "      <th>unknown</th>\n",
       "      <th>action</th>\n",
       "      <th>adventure</th>\n",
       "      <th>animation</th>\n",
       "      <th>childrens</th>\n",
       "      <th>...</th>\n",
       "      <th>fantasy</th>\n",
       "      <th>filmNoir</th>\n",
       "      <th>horror</th>\n",
       "      <th>musical</th>\n",
       "      <th>mystery</th>\n",
       "      <th>romance</th>\n",
       "      <th>sciFi</th>\n",
       "      <th>thriller</th>\n",
       "      <th>war</th>\n",
       "      <th>western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Toy%20Story%2...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>GoldenEye (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?GoldenEye%20(...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Four%20Rooms%...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Get Shorty (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Get%20Shorty%...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Copycat (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Copycat%20(1995)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieID         movieTitle  releaseDate  videoReleaseDate  \\\n",
       "0        1   Toy Story (1995)  01-Jan-1995               NaN   \n",
       "1        2   GoldenEye (1995)  01-Jan-1995               NaN   \n",
       "2        3  Four Rooms (1995)  01-Jan-1995               NaN   \n",
       "3        4  Get Shorty (1995)  01-Jan-1995               NaN   \n",
       "4        5     Copycat (1995)  01-Jan-1995               NaN   \n",
       "\n",
       "                                             IMDbURL  unknown  action  \\\n",
       "0  http://us.imdb.com/M/title-exact?Toy%20Story%2...        0       0   \n",
       "1  http://us.imdb.com/M/title-exact?GoldenEye%20(...        0       1   \n",
       "2  http://us.imdb.com/M/title-exact?Four%20Rooms%...        0       0   \n",
       "3  http://us.imdb.com/M/title-exact?Get%20Shorty%...        0       1   \n",
       "4  http://us.imdb.com/M/title-exact?Copycat%20(1995)        0       0   \n",
       "\n",
       "   adventure  animation  childrens   ...     fantasy  filmNoir  horror  \\\n",
       "0          0          1          1   ...           0         0       0   \n",
       "1          1          0          0   ...           0         0       0   \n",
       "2          0          0          0   ...           0         0       0   \n",
       "3          0          0          0   ...           0         0       0   \n",
       "4          0          0          0   ...           0         0       0   \n",
       "\n",
       "   musical  mystery  romance  sciFi  thriller  war  western  \n",
       "0        0        0        0      0         0    0        0  \n",
       "1        0        0        0      0         1    0        0  \n",
       "2        0        0        0      0         1    0        0  \n",
       "3        0        0        0      0         0    0        0  \n",
       "4        0        0        0      0         1    0        0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fieldsMovies = ['movieID', 'movieTitle', 'releaseDate', 'videoReleaseDate', 'IMDbURL', 'unknown', 'action', 'adventure',\n",
    "          'animation', 'childrens', 'comedy', 'crime', 'documentary', 'drama', 'fantasy', 'filmNoir', 'horror',\n",
    "          'musical', 'mystery', 'romance','sciFi', 'thriller', 'war', 'western']\n",
    "moviesDF = pd.read_csv(os.path.join(MOVIELENS_DIR, 'u.item'), sep='|', names=fieldsMovies, encoding='latin-1')\n",
    "\n",
    "moviesDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load a train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of lines in train: 80000\n",
      "# of lines in test: 20000\n"
     ]
    }
   ],
   "source": [
    "trainDF = pd.read_csv(os.path.join(MOVIELENS_DIR, 'u1.base'), sep='\\t', names=fields)\n",
    "testDF = pd.read_csv(os.path.join(MOVIELENS_DIR, 'u1.test'), sep='\\t', names=fields)\n",
    "\n",
    "# test number of records (total should be 100K)\n",
    "print(\"# of lines in train:\", trainDF.shape[0])\n",
    "print(\"# of lines in test:\", testDF.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building User-to-Item Rating Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildUserItemMatrix(dataset, numUsers, numItems):\n",
    "    # Initialize a of size (numUsers, numItems) to zeros\n",
    "    matrix = np.zeros((numUsers, numItems), dtype=np.int8)\n",
    "    \n",
    "    # Populate the matrix based on the dataset\n",
    "    for (index, userID, itemID, rating, timestamp) in dataset.itertuples():\n",
    "        matrix[userID-1, itemID-1] = rating\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainUserItemMatrix = buildUserItemMatrix(trainDF, numUsers, numItems)\n",
    "testUserItemMatrix = buildUserItemMatrix(testDF, numUsers, numItems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline solution - Average User Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictByUserAverage(trainSet, numUsers, numItems):\n",
    "    # Initialize the predicted rating matrix with zeros\n",
    "    predictionMatrix = np.zeros((numUsers, numItems))\n",
    "    \n",
    "    for (user,item), rating in np.ndenumerate(trainSet):\n",
    "        # Predict rating for every item that wasn't ranked by the user (rating == 0)\n",
    "        if rating == 0:\n",
    "            # Extract the items the user already rated\n",
    "            userVector = trainSet[user, :]\n",
    "            ratedItems = userVector[userVector.nonzero()]\n",
    "            \n",
    "            # If not empty, calculate average and set as rating for the current item\n",
    "            if ratedItems.size == 0:\n",
    "                itemAvg = 0\n",
    "            else:\n",
    "                itemAvg = ratedItems.mean()\n",
    "            predictionMatrix[user, item] = itemAvg\n",
    "            \n",
    "        # report progress every 100 users\n",
    "        if (user % 100 == 0 and item == 1):\n",
    "            print (\"calculated %d users\" % (user,))\n",
    "    \n",
    "    return predictionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n"
     ]
    }
   ],
   "source": [
    "userAvgPreiction = predictByUserAverage(trainUserItemMatrix, numUsers, numItems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How well did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(pred, test):\n",
    "    # calculate RMSE for all the items in the test dataset\n",
    "    predItems = pred[test.nonzero()].flatten() \n",
    "    testItems = test[test.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(predItems, testItems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-User Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userSimilarity = 1 - pairwise_distances(trainUserItemMatrix, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predictByUserSimilarity(trainSet, numUsers, numItems, similarity):\n",
    "    # Initialize the predicted rating matrix with zeros\n",
    "    predictionMatrix = np.zeros((numUsers, numItems))\n",
    "    \n",
    "    for (user,item), rating in np.ndenumerate(trainSet):\n",
    "        # Predict rating for every item that wasn't ranked by the user (rating == 0)\n",
    "        if rating == 0:\n",
    "            # Extract the users that provided rating for this item\n",
    "            itemVector = trainSet[:,item]\n",
    "           \n",
    "            usersRatings = itemVector[itemVector.nonzero()]\n",
    "            \n",
    "            # Get the similarity score for each of the users that provided rating for this item\n",
    "           \n",
    "            usersSim = similarity[user,:][itemVector.nonzero()]\n",
    "            \n",
    "            # If there no users that ranked this item, use user's average\n",
    "            if len(usersSim) == 0:\n",
    "                userVector = trainSet[user, :]\n",
    "                ratedItems = userVector[userVector.nonzero()]\n",
    "                \n",
    "                # If the user didnt rated any item use 0, otherwise use average\n",
    "                if len(ratedItems) == 0:\n",
    "                    predictionMatrix[user,item] = 0\n",
    "                else:\n",
    "                    predictionMatrix[user,item] = ratedItems.mean()\n",
    "            else:\n",
    "                # predict score based on user-user similarity\n",
    "                predictionMatrix[user,item] = (usersRatings*usersSim).sum() / usersSim.sum()\n",
    "        \n",
    "        # report progress every 100 users\n",
    "        if (user % 100 == 0 and item == 1):\n",
    "            print (\"calculated %d users\" % (user,))\n",
    "    \n",
    "\n",
    "    return predictionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated 0 users\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "userSimPreiction = predictByUserSimilarity(trainUserItemMatrix, numUsers, numItems, userSimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.026449013124381"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(userSimPreiction, testUserItemMatrix)\n",
    "\n",
    "print(st.t.interval(0.95, len(popprecision)-1, loc=rmse(userSimPreiction, testUserItemMatrix), scale=st.sem(popprecision)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision@k and Recall@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avgPrecisionAtK(testSet, prediction, k):\n",
    "    # Initialize sum and count vars for average calculation\n",
    "    sumPrecisions = 0\n",
    "    countPrecisions = 0\n",
    "    \n",
    "    # Define function for converting 1-5 rating to 0/1 (like / don't like)\n",
    "    vf = np.vectorize(lambda x: 1 if x >= 4 else 0)\n",
    "    \n",
    "    for userID in range(numUsers):\n",
    "        # Pick top K based on predicted rating\n",
    "        userVector = prediction[userID,:]\n",
    "        topK = nlargest(k, range(len(userVector)), userVector.take)\n",
    "        \n",
    "        # Convert test set ratings to like / don't like\n",
    "        userTestVector = vf(testSet[userID,:]).nonzero()[0]\n",
    "        \n",
    "        # Calculate precision\n",
    "        precision = len([item for item in topK if item in userTestVector])/len(topK)\n",
    "        \n",
    "        # Update sum and count\n",
    "        sumPrecisions += precision\n",
    "        countPrecisions += 1\n",
    "        \n",
    "    # Return average P@k\n",
    "    return sumPrecisions/countPrecisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avgRecallAtK(testSet, prediction, k):\n",
    "    # Initialize sum and count vars for average calculation\n",
    "    sumRecalls = 0\n",
    "    countRecalls = 0\n",
    "    \n",
    "    # Define function for converting 1-5 rating to 0/1 (like / don't like)\n",
    "    vf = np.vectorize(lambda x: 1 if x >= 4 else 0)\n",
    "    \n",
    "    for userID in range(numUsers):\n",
    "        # Pick top K based on predicted rating\n",
    "        userVector = prediction[userID,:]\n",
    "        topK = nlargest(k, range(len(userVector)), userVector.take)\n",
    "        \n",
    "        # Convert test set ratings to like / don't like\n",
    "        userTestVector = vf(testSet[userID,:]).nonzero()[0]\n",
    "        \n",
    "        # Ignore user if has no ratings in the test set\n",
    "        if (len(userTestVector) == 0):\n",
    "            continue\n",
    "        \n",
    "        # Calculate recall\n",
    "        recall = len([item for item in topK if item in userTestVector])/len(userTestVector)\n",
    "        \n",
    "        # Update sum and count\n",
    "        sumRecalls += recall\n",
    "        countRecalls += 1\n",
    "    \n",
    "    # Return average R@k\n",
    "    return sumRecalls/countRecalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"k\\tP@k\\tR@k\")\n",
    "for k in [25, 50, 100, 250, 500]:\n",
    "    print(\"%d\\t%.3lf\\t%.3lf\" % (k, avgPrecisionAtK(testUserItemMatrix, userSimPreiction, k), avgRecallAtK(testUserItemMatrix, userSimPreiction, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Popularity Based Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictByPopularity(trainSet, numUsers, numItems):\n",
    "    # Initialize the predicted rating matrix with zeros\n",
    "    predictionMatrix = np.zeros((numUsers, numItems))\n",
    "    \n",
    "    # Define function for converting 1-5 rating to 0/1 (like / don't like)\n",
    "    vf = np.vectorize(lambda x: 1 if x >= 4 else 0)\n",
    "    \n",
    "    # For every item calculate the number of people liked (4-5) divided by the number of people that rated\n",
    "    itemPopularity = np.zeros((numItems))\n",
    "    for item in range(numItems):\n",
    "        numOfUsersRated = len(trainSet[:, item].nonzero()[0])\n",
    "        numOfUsersLiked = len(vf(trainSet[:, item]).nonzero()[0])\n",
    "        if numOfUsersRated == 0:\n",
    "            itemPopularity[item] = 0\n",
    "        else:\n",
    "            itemPopularity[item] = numOfUsersLiked/numOfUsersRated\n",
    "    \n",
    "    for (user,item), rating in np.ndenumerate(trainSet):\n",
    "        # Predict rating for every item that wasn't ranked by the user (rating == 0)\n",
    "        if rating == 0:\n",
    "            predictionMatrix[user, item] = itemPopularity[item]\n",
    "            \n",
    "        # report progress every 100 users\n",
    "        if (user % 100 == 0 and item == 1):\n",
    "            print (\"calculated %d users\" % (user,))\n",
    "    \n",
    "    return predictionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n"
     ]
    }
   ],
   "source": [
    "popPreiction = predictByPopularity(trainUserItemMatrix, numUsers, numItems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Recommendations for a User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def userTopK(prediction, moviesDataset, userID, k):\n",
    "    # Pick top K based on predicted rating\n",
    "    userVector = prediction[userID+1,:]\n",
    "    topK = nlargest(k, range(len(userVector)), userVector.take)\n",
    "    namesTopK = list(map(lambda x: moviesDataset[moviesDataset.movieID == x+1][\"movieTitle\"].values[0], topK))\n",
    "    return namesTopK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Loch Ness (1995)',\n",
       " 'Perfect Candidate, A (1996)',\n",
       " 'Love and Death on Long Island (1997)',\n",
       " 'Crossfire (1947)',\n",
       " 'Celestial Clockwork (1994)',\n",
       " 'They Made Me a Criminal (1939)',\n",
       " 'Last Time I Saw Paris, The (1954)',\n",
       " 'Innocents, The (1961)',\n",
       " \"Jupiter's Wife (1994)\",\n",
       " 'Prefontaine (1997)']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recommend for userID 350 according to popularity recommender\n",
    "userTopK(popPreiction, moviesDF, 350, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# recommend for userID 350 according to average rating recommender\n",
    "userTopK(userAvgPreiction, moviesDF, 350, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# recommend for userID 350 according to user similarity recommender\n",
    "userTopK(userSimPreiction, moviesDF, 350, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasetsFileNames = [('u1.base', 'u1.test'),  \n",
    "                     ('u2.base', 'u2.test'),\n",
    "                     ('u3.base', 'u3.test'),\n",
    "                     ('u4.base', 'u4.test'),\n",
    "                     ('u5.base', 'u5.test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n"
     ]
    }
   ],
   "source": [
    "simrmseList = []\n",
    "eucrmseList = []\n",
    "manrsmeList = []\n",
    "for trainFileName, testFileName in datasetsFileNames:\n",
    "    curTrainDF = pd.read_csv(os.path.join(MOVIELENS_DIR, trainFileName), sep='\\t', names=fields)\n",
    "    curTestDF = pd.read_csv(os.path.join(MOVIELENS_DIR, testFileName), sep='\\t', names=fields)\n",
    "    curTrainUserItemMatrix = buildUserItemMatrix(curTrainDF, numUsers, numItems)\n",
    "    curTestUserItemMatrix = buildUserItemMatrix(curTestDF, numUsers, numItems)\n",
    "    \n",
    "     \n",
    "    curUserSimilarity = 1 - pairwise_distances(curTrainUserItemMatrix, metric='cosine')\n",
    "    curUserSimPreiction = predictByUserSimilarity(curTrainUserItemMatrix, numUsers, numItems, curUserSimilarity)\n",
    "    simRMSE = rmse(curUserSimPreiction, curTestUserItemMatrix)\n",
    "    simrmseList.append(simRMSE)\n",
    "    \n",
    "    cureucSimilarity = 1/(1 + pairwise_distances(curTrainUserItemMatrix, metric='euclidean'))\n",
    "    cureucSimPreiction = predictByUserSimilarity(curTrainUserItemMatrix, numUsers, numItems, cureucSimilarity)\n",
    "    eucRMSE = rmse(cureucSimPreiction, curTestUserItemMatrix)\n",
    "    eucrmseList.append(eucRMSE)\n",
    "    \n",
    "    curmanSimilarity = 1 - (pairwise_distances(curTrainUserItemMatrix, metric='manhattan') * .1)\n",
    "    curmanSimPreiction = predictByUserSimilarity(curTrainUserItemMatrix, numUsers, numItems, curmanSimilarity)\n",
    "    manRMSE = rmse(curmanSimPreiction, curTestUserItemMatrix)\n",
    "    manrsmeList.append(manRMSE)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.02692517384\n",
      "1.0224877621\n",
      "1.01735412166\n",
      "(1.0198755618891411, 1.0339747857902604)\n",
      "(1.013706565892023, 1.031268958300787)\n",
      "(1.0090130802261479, 1.0256951630950137)\n"
     ]
    }
   ],
   "source": [
    "manavg = np.mean(manrsmeList)\n",
    "eucavg = np.mean(eucrmseList)\n",
    "cosavg = np.mean(simrmseList)\n",
    "\n",
    "CIman = st.t.interval(0.95, len(manrsmeList)-1, loc=np.mean(manrsmeList), scale=st.sem(manrsmeList))\n",
    "CIeuc = st.t.interval(0.95, len(eucrmseList)-1, loc=np.mean(eucrmseList), scale=st.sem(eucrmseList))\n",
    "CIcos = st.t.interval(0.95, len(simrmseList)-1, loc=np.mean(simrmseList), scale=st.sem(simrmseList))\n",
    "\n",
    "print(manavg)\n",
    "print(eucavg)\n",
    "print(cosavg)\n",
    "print(CIman)\n",
    "print(CIeuc)\n",
    "print(CIcos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Item-Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itemSimilarity = 1 - (pairwise_distances(trainUserItemMatrix.T, metric='cosine') )\n",
    "def predictByItemSimilarity(trainSet, numUsers, numItems, similarity):\n",
    "    # Initialize the predicted rating matrix with zeros\n",
    "    predictionMatrix = np.zeros((numItems, numUsers))\n",
    "    \n",
    "    for (user,item), rating in np.ndenumerate(trainSet):\n",
    "        # Predict rating for every user that wasn't ranked by the user (rating == 0)\n",
    "        if rating == 0:\n",
    "            # Extract the users that provided rating for this item\n",
    "            itemVector = trainSet[:,item]\n",
    "            \n",
    "            usersRatings = itemVector[itemVector.nonzero()]\n",
    "            \n",
    "            # Get the similarity score for each of the items that provided rating for this item\n",
    "           \n",
    "            usersSim = similarity[user,:][itemVector.nonzero()]\n",
    "             \n",
    "            # If there no items that were ranked by this user, use item's average\n",
    "            if len(usersSim) == 0:\n",
    "                userVector = trainSet[user, :]\n",
    "                ratedItems = userVector[userVector.nonzero()]\n",
    "                \n",
    "                # If the items werent rated use 0, otherwise use average\n",
    "                if len(ratedItems) == 0:\n",
    "                    predictionMatrix[user,item] = 0\n",
    "                else:\n",
    "                    predictionMatrix[user,item] = ratedItems.mean()\n",
    "            else:\n",
    "                # predict score based on item-item similarity\n",
    "                if(usersSim.sum() == 0):\n",
    "                    predictionMatrix[user,item] = 0\n",
    "                else:\n",
    "                    predictionMatrix[user,item] = (usersRatings*usersSim).sum() / usersSim.sum()\n",
    "        \n",
    "        # report progress every 100 users\n",
    "        if (user % 100 == 0 and item == 1):\n",
    "            print (\"calculated %d users\" % (user,))\n",
    "    \n",
    "\n",
    "    return predictionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 1000 users\n",
      "calculated 1100 users\n",
      "calculated 1200 users\n",
      "calculated 1300 users\n",
      "calculated 1400 users\n",
      "calculated 1500 users\n",
      "calculated 1600 users\n"
     ]
    }
   ],
   "source": [
    "itemSimPreiction = predictByItemSimilarity(trainUserItemMatrix.T, numUsers, numItems, itemSimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0377631264364242"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(itemSimPreiction, testUserItemMatrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def userTopK(prediction, moviesDataset, userID, k):\n",
    "    # Pick top K based on predicted rating\n",
    "    userVector = prediction[userID+1,:]\n",
    "    topK = nlargest(k, range(len(userVector)), userVector.take)\n",
    "    namesTopK = list(map(lambda x: moviesDataset[moviesDataset.movieID == x+1][\"movieTitle\"].values[0], topK))\n",
    "    return namesTopK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Movies Using Item-Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Venice/Venice (1992)',\n",
       " 'Chairman of the Board (1998)',\n",
       " 'Chairman of the Board (1998)',\n",
       " 'Santa with Muscles (1996)',\n",
       " 'Ballad of Narayama, The (Narayama Bushiko) (1958)',\n",
       " 'Falling in Love Again (1980)',\n",
       " 'Convent, The (Convento, O) (1995)',\n",
       " 'Letter From Death Row, A (1998)',\n",
       " 'Innocent Sleep, The (1995)',\n",
       " 'Turning, The (1992)']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userTopK(itemSimPreiction, moviesDF, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scream of Stone (Schrei aus Stein) (1991)\n",
      "Other Voices, Other Rooms (1997)\n",
      "Big Bang Theory, The (1994)\n",
      "Chairman of the Board (1998)\n",
      "Marked for Death (1990)\n"
     ]
    }
   ],
   "source": [
    "movie = userSimilarity[:,5]\n",
    "\n",
    "movie = np.argsort(movie)\n",
    "\n",
    "for a in range(5):\n",
    "    \n",
    "    print(moviesDF.movieTitle[movie[a]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 1000 users\n",
      "calculated 1100 users\n",
      "calculated 1200 users\n",
      "calculated 1300 users\n",
      "calculated 1400 users\n",
      "calculated 1500 users\n",
      "calculated 1600 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 1000 users\n",
      "calculated 1100 users\n",
      "calculated 1200 users\n",
      "calculated 1300 users\n",
      "calculated 1400 users\n",
      "calculated 1500 users\n",
      "calculated 1600 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 1000 users\n",
      "calculated 1100 users\n",
      "calculated 1200 users\n",
      "calculated 1300 users\n",
      "calculated 1400 users\n",
      "calculated 1500 users\n",
      "calculated 1600 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 1000 users\n",
      "calculated 1100 users\n",
      "calculated 1200 users\n",
      "calculated 1300 users\n",
      "calculated 1400 users\n",
      "calculated 1500 users\n",
      "calculated 1600 users\n",
      "calculated 0 users\n",
      "calculated 100 users\n",
      "calculated 200 users\n",
      "calculated 300 users\n",
      "calculated 400 users\n",
      "calculated 500 users\n",
      "calculated 600 users\n",
      "calculated 700 users\n",
      "calculated 800 users\n",
      "calculated 900 users\n",
      "calculated 1000 users\n",
      "calculated 1100 users\n",
      "calculated 1200 users\n",
      "calculated 1300 users\n",
      "calculated 1400 users\n",
      "calculated 1500 users\n",
      "calculated 1600 users\n"
     ]
    }
   ],
   "source": [
    "rmseList2 = []\n",
    "for trainFileName, testFileName in datasetsFileNames:\n",
    "    curTrainDF = pd.read_csv(os.path.join(MOVIELENS_DIR, trainFileName), sep='\\t', names=fields)\n",
    "    curTestDF = pd.read_csv(os.path.join(MOVIELENS_DIR, testFileName), sep='\\t', names=fields)\n",
    "    curTrainUserItemMatrix = buildUserItemMatrix(curTrainDF, numUsers, numItems)\n",
    "    curTestUserItemMatrix = buildUserItemMatrix(curTestDF, numUsers, numItems)\n",
    "    \n",
    "     \n",
    "    curUserSimilarity = 1 - pairwise_distances(curTrainUserItemMatrix.T, metric='cosine')\n",
    "    curUserSimPreiction = predictByItemSimilarity(curTrainUserItemMatrix.T, numUsers, numItems, curUserSimilarity)\n",
    "    simRMSE = rmse(curUserSimPreiction, curTestUserItemMatrix.T)\n",
    "    \n",
    "    rmseList2.append(simRMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE Stat for UserAvgPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmseList3 = []\n",
    "for trainFileName, testFileName in datasetsFileNames:\n",
    "    curTrainDF = pd.read_csv(os.path.join(MOVIELENS_DIR, trainFileName), sep='\\t', names=fields)\n",
    "    curTestDF = pd.read_csv(os.path.join(MOVIELENS_DIR, testFileName), sep='\\t', names=fields)\n",
    "    curTrainUserItemMatrix = buildUserItemMatrix(curTrainDF, numUsers, numItems)\n",
    "    curTestUserItemMatrix = buildUserItemMatrix(curTestDF, numUsers, numItems)\n",
    "    userAvgPreiction = predictByUserAverage(curTrainUserItemMatrix, numUsers, numItems)\n",
    "    rmseList3.append(rmse(userAvgPreiction, curTestUserItemMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.04371765616\n",
      "(1.0289303496379316, 1.0585049626810734)\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(rmseList3))\n",
    "\n",
    "print(st.t.interval(0.95, len(rmseList3)-1, loc=np.mean(rmseList3), scale=st.sem(rmseList3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
